{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import findspark\n",
    "import numpy as np\n",
    "from argparse import ArgumentParser\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nparser = ArgumentParser(description='PySpark NLP Intro')\\nparser.add_argument('--minTokenLength', type=int, default=2, metavar='b',\\n                    help='The minimum length of any accepted tokens; tokens that are too short will be dropped')\\nparser.add_argument('--minInitialCount', type=int, default=2, metavar='b',\\n                    help='The minimum number of times a token must appear in a document                    to be considered for the global calculation')\\nparser.add_argument('--textDir', type=str, default='.', metavar='D',\\n                    help='The directory from which to pull texts')\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "parser = ArgumentParser(description='PySpark NLP Intro')\n",
    "parser.add_argument('--minTokenLength', type=int, default=2, metavar='b',\n",
    "                    help='The minimum length of any accepted tokens; tokens that are too short will be dropped')\n",
    "parser.add_argument('--minInitialCount', type=int, default=2, metavar='b',\n",
    "                    help='The minimum number of times a token must appear in a document\\\n",
    "                    to be considered for the global calculation')\n",
    "parser.add_argument('--textDir', type=str, default='.', metavar='D',\n",
    "                    help='The directory from which to pull texts')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds and adds spark to python path\n",
    "# Convenient for env managers like conda\n",
    "\n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates an initial spark configuration utilizing all local cores\n",
    "conf = SparkConf().setMaster(\"local[*]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates spark context through which to process RDD ops\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set main data directory\n",
    "directory='data'\n",
    "\n",
    "# Tokenize and make case-insensitive the stopwords, then collect into a list\n",
    "stopwords=sc.broadcast(sc.textFile(directory+'/stopwords.txt')\\\n",
    "                .flatMap(lambda x: x.strip().split())\\\n",
    "                .map(lambda x:x.lower())\\\n",
    "                .collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Defines ops for a single document. Specifically we:\n",
    "flatMap using .strip().split() to tokenize the initial textfile,\n",
    "map x->(x.lower(),1) to both render the process case insensitive\n",
    "while also conforming to key:value format for further ops. Next\n",
    "we reduceByKey to calculate a 'word count' based on the tokenization.\n",
    "Finally we filter to retain only words/tokens that appear more than once.\n",
    "This returns the filtered RDD itself for further handling later.\n",
    "\n",
    "'''\n",
    "def singleDocSC1(file,stopwords):\n",
    "    data=sc.textFile(file)\\\n",
    "        .flatMap(lambda x: x.strip().split())\\\n",
    "        .map(lambda x:(x.lower(),1))\\\n",
    "        .reduceByKey(lambda x, y: x + y)\\\n",
    "        .filter(lambda x:x[1]>1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Same as the SC1 variant, except this introduces stopwords\n",
    "'''\n",
    "def singleDocSC2(file,stopwords):\n",
    "    data=sc.textFile(file)\\\n",
    "        .flatMap(lambda x: x.strip().split())\\\n",
    "        .map(lambda x:(x.lower(),1))\\\n",
    "        .reduceByKey(lambda x, y: x + y)\\\n",
    "        .filter(lambda x:x[1]>1 and x[0] not in stopwords)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _strip(x,full=False):\n",
    "    if full:\n",
    "        x=x.strip(punctuation)\n",
    "    else:\n",
    "        punc='.,:;!?\\''\n",
    "        if x[0] in punc: x=x[1:]\n",
    "        if x[-1] in punc: x=x[:-1]\n",
    "    return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Same as the SC2 variant, except this discards tokens of len==1\n",
    "and strips trailing and leading punctuation\n",
    "'''\n",
    "def singleDocSC3(file,stopwords):\n",
    "    data=sc.textFile(file)\\\n",
    "        .flatMap(lambda x: x.strip().split())\\\n",
    "        .filter(lambda x:len(x)>1)\\\n",
    "        .map(lambda x:(_strip(x).lower() ,1))\\\n",
    "        .reduceByKey(lambda x, y: x + y)\\\n",
    "        .filter(lambda x:x[1]>1 and x[0] not in stopwords)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(func,outName):\n",
    "    \n",
    "    # Creates an empty list which is then filled by the processed RDD of each doc\n",
    "    docs=[]\n",
    "    \n",
    "    # Set a basic loop over the data directory to catch all relevant txt files\n",
    "    # Note that only one txt file didn't start with 'pg', so for simplicity I renamed it\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.startswith(\"pg\"): \n",
    "            docs.append(func(os.path.join(directory, filename),stopwords.value))\n",
    "            print(f'processing: {filename}')\n",
    "    # Initializes the final result using the first doc in the list\n",
    "    out=docs[0]\n",
    "\n",
    "    # For each doc in the list after master, merge with master\n",
    "    print(f'merging')\n",
    "    for d in docs[1:]:\n",
    "        out=out.union(d)\n",
    "\n",
    "    # Reduce to achieve a 'net count' accross docs\n",
    "    out=out.reduceByKey(lambda x,y : x+y)\n",
    "    print('reducing')\n",
    "\n",
    "    # JSON dump the final, reduced master, which now contains the token count across ALL docs\n",
    "    print('dumping')\n",
    "    with open(outName+'.json', 'w') as f:\n",
    "        json.dump(dict(out.top(40, key=lambda x: x[1])), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing: pg1497.txt\n",
      "processing: pg19033.txt\n",
      "processing: pg3207.txt\n",
      "processing: pg36.txt\n",
      "processing: pg42671.txt\n",
      "processing: pg4300-0.txt\n",
      "processing: pg514.txt\n",
      "processing: pg6130.txt\n",
      "merging\n",
      "reducing\n",
      "dumping\n"
     ]
    }
   ],
   "source": [
    "#main(singleDocSC1,'sp1')\n",
    "#main(singleDocSC2,'sp2')\n",
    "main(singleDocSC3,'sp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Identical to SC3\n",
    "'''\n",
    "def singleDocSC4(file,stopwords):\n",
    "    data=sc.textFile(file)\\\n",
    "        .flatMap(lambda x: x.strip().split())\\\n",
    "        .filter(lambda x:len(x)>1)\\\n",
    "        .map(lambda x:(_strip(x).lower() ,1))\\\n",
    "        .reduceByKey(lambda x, y: x + y)\\\n",
    "        .filter(lambda x:x[1]>1 and x[0] not in stopwords)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mainTF_IDF(func,outName):\n",
    "    # Number of documents, used to calculate IDF\n",
    "    N=8\n",
    "    \n",
    "    # Creates an empty list which is then filled by the processed RDD of each doc\n",
    "    docs=[]\n",
    "    \n",
    "    # Set a basic loop over the data directory to catch all relevant txt files\n",
    "    # Note that only one txt file didn't start with 'pg', so for simplicity I renamed it\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.startswith(\"pg\"): \n",
    "            docs.append(func(os.path.join(directory, filename),stopwords.value))\n",
    "            print(f'processing: {filename}')\n",
    "    # Retain a master list of all words used across each document, as well as\n",
    "    # the number of documents that they are present in\n",
    "    master=docs[0].map(lambda x:(x[0],1))\n",
    "    print('Tallying IDF contribution from doc #0')\n",
    "            \n",
    "    # For each doc in the list, add (union) its tokens to the master list with value 1\n",
    "    for i in range(len(docs)-1):\n",
    "        master=master.union(docs[i+1].map(lambda x:(x[0],1)))\n",
    "        print(f'Tallying IDF contribution from doc #{i+1}')\n",
    "\n",
    "    # Take the sum of the values of each key, thereby calculating the number of documents\n",
    "    # that each token is present in. Then, map to calculate IDF\n",
    "    print(f'Calculating IDF')\n",
    "    master=master.reduceByKey(lambda x,y:x+y)\\\n",
    "                 .map(lambda x:(x[0],np.log(N/x[1])))\\\n",
    "                 .cache()\n",
    "    \n",
    "    print(f'Converting to dict and broadcasting')\n",
    "    # Convert the TF-IDF list into a dictionary\n",
    "    weights=sc.broadcast(master.collectAsMap())\n",
    "    \n",
    "    # Using the TF, calculate the TF-IDF for each document\n",
    "    for i in range(len(docs)):\n",
    "        print(f'Weighting document {i}')\n",
    "        docs[i]=docs[i].join.map(lambda x:(x[0],x[1]*weights.value[x[0]]))\n",
    "    \n",
    "    # Collect the top 5 words in each doc by TF-IDF score\n",
    "    out=[]\n",
    "    for d in docs:\n",
    "        out+=d.top(5, key=lambda x: x[1])\n",
    "\n",
    "    # JSON dump the final list of the top 40 TF-IDF scores\n",
    "    with open(outName+'.json', 'w') as f:\n",
    "        json.dump(dict(out), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing: pg1497.txt\n",
      "processing: pg19033.txt\n",
      "processing: pg3207.txt\n",
      "processing: pg36.txt\n",
      "processing: pg42671.txt\n",
      "processing: pg4300-0.txt\n",
      "processing: pg514.txt\n",
      "processing: pg6130.txt\n",
      "Tallying IDF contribution from doc #0\n",
      "Tallying IDF contribution from doc #1\n",
      "Tallying IDF contribution from doc #2\n",
      "Tallying IDF contribution from doc #3\n",
      "Tallying IDF contribution from doc #4\n",
      "Tallying IDF contribution from doc #5\n",
      "Tallying IDF contribution from doc #6\n",
      "Tallying IDF contribution from doc #7\n",
      "Calculating IDF\n",
      "Converting to dict and broadcasting\n",
      "Weighting document 0\n",
      "Weighting document 1\n",
      "Weighting document 2\n",
      "Weighting document 3\n",
      "Weighting document 4\n",
      "Weighting document 5\n",
      "Weighting document 6\n",
      "Weighting document 7\n"
     ]
    }
   ],
   "source": [
    "mainTF_IDF(singleDocSC4,'sp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A modification of SC3/4 that performs a more comprehensive, well-behaved strip\n",
    "'''\n",
    "def singleDocSC5(file,stopwords):\n",
    "    data=sc.textFile(file)\\\n",
    "        .flatMap(lambda x: x.strip().split())\\\n",
    "        .map(lambda x:(_strip(x.lower(),True) ,1))\\\n",
    "        .reduceByKey(lambda x, y: x + y)\\\n",
    "        .filter(lambda x:x[1]>1 and x[0] not in stopwords)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing: pg1497.txt\n",
      "processing: pg19033.txt\n",
      "processing: pg3207.txt\n",
      "processing: pg36.txt\n",
      "processing: pg42671.txt\n",
      "processing: pg4300-0.txt\n",
      "processing: pg514.txt\n",
      "processing: pg6130.txt\n",
      "Tallying IDF contribution from doc #0\n",
      "Tallying IDF contribution from doc #1\n",
      "Tallying IDF contribution from doc #2\n",
      "Tallying IDF contribution from doc #3\n",
      "Tallying IDF contribution from doc #4\n",
      "Tallying IDF contribution from doc #5\n",
      "Tallying IDF contribution from doc #6\n",
      "Tallying IDF contribution from doc #7\n",
      "Calculating IDF\n",
      "Converting to dict and broadcasting\n",
      "Weighting document 0\n",
      "Weighting document 1\n",
      "Weighting document 2\n",
      "Weighting document 3\n",
      "Weighting document 4\n",
      "Weighting document 5\n",
      "Weighting document 6\n",
      "Weighting document 7\n"
     ]
    }
   ],
   "source": [
    "mainTF_IDF(singleDocSC5,'sp5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
